{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a909f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03e5f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the dimensions of the volume\n",
    "width = 181\n",
    "height = 217\n",
    "depth = 181\n",
    "\n",
    "# Open the input file in binary mode\n",
    "with open(\"BrainWeb/t1_icbm_normal_1mm_pn7_rf20.rawb\", \"rb\") as file:\n",
    "    # Loop through each slice in the volume\n",
    "    for z in range(depth):\n",
    "        # Create a new PGM file for the slice\n",
    "        with open(f'Extracted_files//pgm//t1_icbm_normal_1mm_pn7_rf20//slice_{z}.pgm', \"w+\") as pgm_file:\n",
    "            # Write the PGM file header\n",
    "            pgm_file.write(\"P2\\n\")\n",
    "            pgm_file.write(\"#\\n\")\n",
    "            pgm_file.write(f\"{width} {height}\\n\")\n",
    "            pgm_file.write(\"255\\n\")\n",
    "\n",
    "            # Loop through each row in the slice\n",
    "            for y in range(height):\n",
    "                # Read a row of data from the input file\n",
    "                row_data = file.read(width)  # Each value is 1 byte\n",
    "                # Unpack the row data into a list of integers\n",
    "                values = struct.unpack(f\"{width}B\", row_data)\n",
    "                \n",
    "                # Find the maximum value in the input data\n",
    "                max_value = max(values)\n",
    "                \n",
    "                if max_value == 0:\n",
    "                    # Handle the case where all values are zero\n",
    "                    scaled_values = [0] * width\n",
    "                else:\n",
    "                    # Scale the values down to the maximum range of 255\n",
    "                    #scaled_values = [int(v) if v <= 3 else 0 for v in values]\n",
    "                    #scaled_values = [int(v * 255 / max_value) for v in values]\n",
    "                    scaled_values = [int(v) for v in values]\n",
    "\n",
    "                # Convert each scaled value to decimal and write it to the PGM file\n",
    "                pgm_file.write(\" \".join([str(v) for v in scaled_values]) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3aefaec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the input and output directories\n",
    "input_dir = \"Extracted_files/pgm/t1_icbm_normal_1mm_pn7_rf20\"\n",
    "output_dir = \"Extracted_files/jpg/t1_icbm_normal_1mm_pn7_rf20\"\n",
    "\n",
    "# Loop through all of the PGM files in the input directory\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".pgm\"):\n",
    "        # Open the PGM file\n",
    "        with Image.open(os.path.join(input_dir, file)) as img:\n",
    "            # Convert the PGM file to a JPEG file\n",
    "            img.save(os.path.join(output_dir, os.path.splitext(file)[0] + \".jpg\"), \"JPEG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load and preprocess images from folder\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "# Load original MRI images\n",
    "original_images = load_images_from_folder(\"ground_truth\")\n",
    "\n",
    "# Load discrete images\n",
    "discrete_images = load_images_from_folder(\"original\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b96ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists of images to numpy arrays\n",
    "original_images = np.array(original_images)\n",
    "discrete_images = np.array(discrete_images)\n",
    "\n",
    "# Flatten images\n",
    "original_images_flat = original_images.reshape((original_images.shape[0], -1))\n",
    "discrete_images_flat = discrete_images.reshape((discrete_images.shape[0], -1))\n",
    "\n",
    "# Preprocessing: Normalize original images\n",
    "original_images_flat_normalized = original_images_flat / 255.0\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(original_images_flat_normalized, discrete_images_flat, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6ef89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3731c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF Model Setup\n",
    "num_clusters = 10  # Number of clusters (centers) for RBF units\n",
    "\n",
    "# Initialize RBF centers using k-means clustering on original images\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(X_train)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Define the spread parameter (variance) for each RBF unit\n",
    "spread = 0.1  # Example spread parameter, adjust as needed\n",
    "\n",
    "# Compute RBF activations for each training sample and center\n",
    "rbf_activations_train = np.exp(-np.sum((X_train[:, np.newaxis] - centers) ** 2, axis=2) / (2 * spread ** 2))\n",
    "\n",
    "# Train output layer using Ridge Classifier\n",
    "clf = RidgeClassifier()\n",
    "\n",
    "# MultiOutputClassifier to handle multi-output classification\n",
    "multi_output_clf = MultiOutputClassifier(clf)\n",
    "multi_output_clf.fit(rbf_activations_train, y_train)\n",
    "\n",
    "# Compute RBF activations for test images\n",
    "rbf_activations_test = np.exp(-np.sum((X_test[:, np.newaxis] - centers) ** 2, axis=2) / (2 * spread ** 2))\n",
    "\n",
    "# Make predictions\n",
    "predictions = multi_output_clf.predict(rbf_activations_test)\n",
    "\n",
    "# Evaluate each output separately\n",
    "for i in range(discrete_images_flat.shape[1]):\n",
    "    print(f\"Output {i + 1} classification report:\")\n",
    "    print(classification_report(y_test[:, i], predictions[:, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb16473",
   "metadata": {},
   "source": [
    "For all pixel values from all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de79168",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 217\n",
    "width = 181\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for t in range(40):\n",
    "    k = 0  # Reset the index for each file\n",
    "    l = 0  # Reset the index for each file\n",
    "    \n",
    "    # Load pixel values for the current file\n",
    "    train_pixel_values_t = locals().get(f'train_pixel_values_{t}', None)\n",
    "    if train_pixel_values_t is None:\n",
    "        continue  # Skip if data for current file is not found\n",
    "    \n",
    "    # Load mask pixel values for the current file\n",
    "    train_mask_pixel_values_t = locals().get(f'train_mask_pixel_values_{t}', None)\n",
    "    if train_mask_pixel_values_t is None:\n",
    "        continue  # Skip if data for current file is not found\n",
    "    \n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            rows = []\n",
    "            rows.append(train_pixel_values_t[k])\n",
    "            X_train.append(rows)\n",
    "            k += 1\n",
    "            \n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            y_train.append(train_mask_pixel_values_t[l])\n",
    "            l += 1\n",
    "            \n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for t in range(11):\n",
    "    k = 0  # Reset the index for each file\n",
    "    l = 0  # Reset the index for each file\n",
    "    \n",
    "    # Load pixel values for the current file\n",
    "    test_pixel_values_t = locals().get(f'test_pixel_values_{t}', None)\n",
    "    if test_pixel_values_t is None:\n",
    "        continue  # Skip if data for current file is not found\n",
    "    \n",
    "    # Load mask pixel values for the current file\n",
    "    test_mask_pixel_values_t = locals().get(f'test_mask_pixel_values_{t}', None)\n",
    "    if test_mask_pixel_values_t is None:\n",
    "        continue  # Skip if data for current file is not found\n",
    "    \n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            rows = []\n",
    "            rows.append(test_pixel_values_t[k])\n",
    "            X_test.append(rows)\n",
    "            k += 1\n",
    "            \n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            y_test.append(test_mask_pixel_values_t[l])\n",
    "            l += 1\n",
    "            \n",
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a57d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf')\n",
    "\n",
    "#Fit the model to the training data\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "cm_display.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
